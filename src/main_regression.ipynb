{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from utils.sampling import mnist_noniid, cifar10_noniid, mnist_iid, cifar_iid\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from models.Update import LocalUpdate\n",
    "from models.Fed import FedAvg\n",
    "from models.test import test_img, test_img_poison\n",
    "from models.Nets import LogisticRegression, SimpleCNN, ImprovedSimpleCNN\n",
    "\n",
    "from attacks import sign_flipping_attack, additive_noise, GaussianNoise, LIT_attack\n",
    "from aggregations import aggregation\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed = 42\n",
    "setup_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_to_remove = [arg for arg in sys.argv if arg.startswith('--')]\n",
    "for arg in args_to_remove:\n",
    "    sys.argv.remove(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# federated arguments\n",
    "parser.add_argument('--epochs', type=int, default=150, help=\"rounds of training\")\n",
    "\n",
    "parser.add_argument('--num_users', type=int, default=100, help=\"number of users: K\")\n",
    "parser.add_argument('--sample_users', type=int, default=100, help=\"number of users in federated learning C\")\n",
    "parser.add_argument('--attack_ratio', type=float, default=0.5, help= \"ratio of attacker in sampled users\")\n",
    "parser.add_argument('--attack_mode', type=str, default=\"sign\", choices=[\"sign\", \"noise\", \"\"], help=\"type of attack\")\n",
    "parser.add_argument('--aggregation', type=str, default=\"MKrum\", choices=[\"FedAvg\", \"vaegan\", \"atten\", \"Krum\", \"GeoMed\", \"gan\", \"netg\"], help=\"name of aggregation method\")\n",
    "parser.add_argument('--test_label_acc', action='store_true', help='obtain acc of each label and poinson acc')\n",
    "parser.add_argument('--vae_model', type=str, default=\"./VAE_data/netg_fashionmnist3005.pth\", help=\"directory of vae_model for detection\")\n",
    "\n",
    "parser.add_argument('--local_ep', type=int, default=5, help=\"the number of local epochs: E\")\n",
    "parser.add_argument('--local_bs', type=int, default=128, help=\"local batch size: B\")\n",
    "parser.add_argument('--bs', type=int, default=128, help=\"test batch size\")\n",
    "parser.add_argument('--lr', type=float, default=0.01, help=\"learning rate\")\n",
    "parser.add_argument('--momentum', type=float, default=0.5, help=\"SGD momentum (default: 0.5)\")\n",
    "\n",
    "# other arguments\n",
    "parser.add_argument('--dataset', type=str, default='mnist', help=\"name of dataset\")\n",
    "parser.add_argument('--gpu', type=int, default=0, help=\"GPU ID, -1 for CPU\")\n",
    "parser.add_argument('--verbose', action='store_true', help='verbose print')\n",
    "parser.add_argument('--seed', type=int, default=42, help='random seed (default: 1)')\n",
    "\n",
    "parser.add_argument('--isize', type=int, default=32, help='input image size.')\n",
    "parser.add_argument('--channels', type=int, default=1, help='channels of totual data')\n",
    "parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')\n",
    "parser.add_argument('--nc', type=int, default=1, help='input image channels')\n",
    "parser.add_argument('--ndf', type=int, default=64)\n",
    "parser.add_argument('--ngpu', type=int, default=4, help='number of GPUs to use')\n",
    "parser.add_argument('--extralayers', type=int, default=0, help='Number of extra layers on gen and disc')\n",
    "parser.add_argument('--ngf', type=int, default=64)\n",
    "parser.add_argument('--w_bce', type=float, default=1, help='alpha to weight bce loss.')\n",
    "parser.add_argument('--w_rec', type=float, default=50, help='alpha to weight reconstruction loss')\n",
    "parser.add_argument('--w_enc', type=float, default=1, help='alpha to weight encoder loss')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and split users\n",
    "# trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "# dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fashion_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.2860,), (0.3530,))])\n",
    "dataset_train = datasets.FashionMNIST('../data/fashionmnist/', train=True, download=True, transform=trans_fashion_mnist)\n",
    "dataset_test = datasets.FashionMNIST('../data/fashionmnist/', train=False, download=True, transform=trans_fashion_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample users\n",
    "dict_users = cifar_iid(dataset_train, args.sample_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample users\n",
    "# dict_users = mnist_iid(dataset_test, args.sample_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "net_glob = LogisticRegression(input_size, num_classes).to(args.device)\n",
    "# net_glob = ImprovedSimpleCNN().to(args.device)\n",
    "net_glob.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy weights\n",
    "w_glob = net_glob.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "loss_train_epoch = []\n",
    "acc_test_list = []\n",
    "weights_list = []\n",
    "round_times = []  \n",
    "DAR_list = []  \n",
    "DPR_list = []  \n",
    "RR_list = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(args.epochs):\n",
    "    start_time = datetime.now() \n",
    "    w_locals, loss_locals = [], []\n",
    "    # m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), args.sample_users, replace=False)\n",
    "    print(\"Randomly selected {}/{} users for federated learning. {}\".format(args.sample_users, args.num_users, datetime.now().strftime(\"%H:%M:%S\")))\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "    # attack\n",
    "    attacker_num  = int( args.attack_ratio * args.sample_users)\n",
    "    attacker_idxs = np.random.choice(range(args.sample_users), attacker_num, replace=False)\n",
    "    print( \"{}/{} are attackers with {} attack\".format(attacker_num, args.sample_users, args.attack_mode) )\n",
    "    print(\"Attacker idxs: \", np.sort(attacker_idxs))\n",
    "    \n",
    "    for attacker_idx in attacker_idxs:\n",
    "        if args.attack_mode == \"sign\":\n",
    "            w_locals[attacker_idx] = sign_flipping_attack(w_locals[attacker_idx])\n",
    "        elif args.attack_mode == \"noise\":\n",
    "            w_locals[attacker_idx] = GaussianNoise(w_locals[attacker_idx], args)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    if args.attack_mode == \"lit\":\n",
    "        w_mal = []\n",
    "        for attacker_idx in attacker_idxs:\n",
    "            w_mal.append(w_locals[attacker_idx])\n",
    "        w_mal = LIT_attack(w_mal, w_glob, args)\n",
    "\n",
    "    # update global weights\n",
    "    user_sizes = np.array([ len(dict_users[idx]) for idx in idxs_users ])\n",
    "    user_weights = user_sizes / float(sum(user_sizes))\n",
    "    if args.aggregation == \"FedAvg\":\n",
    "        w_glob = FedAvg(w_locals, user_weights)\n",
    "    else:\n",
    "        w_glob, DAR, DPR, RR = aggregation(w_locals, user_weights, args, attacker_idxs, w_glob, dataset_test)\n",
    "        # w_glob = aggregation(w_locals, user_weights, args, attacker_idxs, w_glob, dataset_test)\n",
    "    \n",
    "    DAR_list.append(DAR)\n",
    "    DPR_list.append(DPR)\n",
    "    RR_list.append(RR)\n",
    "    \n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss\n",
    "    loss_avg = np.sum(loss_locals * user_weights)\n",
    "\n",
    "    print('=== Round {:3d}, Average loss {:.6f} ==='.format(iteration+1, loss_avg))\n",
    "    print(\"{} users; time {}\".format(len(idxs_users), datetime.now().strftime(\"%H:%M:%S\")) )\n",
    "\n",
    "    acc_test, loss_test = test_img(copy.deepcopy(net_glob).to(args.device), dataset_test, args)\n",
    "    print(\"Testing accuracy:  {:.2f}, loss: {}\".format(acc_test, loss_test))\n",
    "    acc_test_list.append(acc_test)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    round_duration = end_time - start_time  \n",
    "    round_times.append(round_duration.total_seconds())\n",
    "    print(\"Test end {}\".format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "\n",
    "    loss_train_epoch.append(loss_avg)\n",
    "\n",
    "print(\"=== End ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_test_list, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(DAR_list, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(DPR_list, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(RR_list, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_train_epoch, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(round_times, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './acc_test_results'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "file_name = f\"{directory}/fashionmnist_aggregation_{args.aggregation}_attackmode_{args.attack_mode}_attackratio_{args.attack_ratio}.npy\"\n",
    "np.save(file_name, acc_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './DAR_results'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "file_name = f\"{directory}/fashionmnist_aggregation_{args.aggregation}_attackmode_{args.attack_mode}_attackratio_{args.attack_ratio}.npy\"\n",
    "np.save(file_name, DAR_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './DPR_results'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "file_name = f\"{directory}/fashionmnist_aggregation_{args.aggregation}_attackmode_{args.attack_mode}_attackratio_{args.attack_ratio}.npy\"\n",
    "np.save(file_name, DPR_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './RR_results'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "file_name = f\"{directory}/fashionmnist_aggregation_{args.aggregation}_attackmode_{args.attack_mode}_attackratio_{args.attack_ratio}.npy\"\n",
    "np.save(file_name, RR_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './loss_train_epoch_results'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "file_name = f\"{directory}/fashionmnist_aggregation_{args.aggregation}_attackmode_{args.attack_mode}_attackratio_{args.attack_ratio}.npy\"\n",
    "np.save(file_name, loss_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './round_times_results'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "file_name = f\"{directory}/fashionmnist_aggregation_{args.aggregation}_attackmode_{args.attack_mode}_attackratio_{args.attack_ratio}.npy\"\n",
    "np.save(file_name, round_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
